{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joaoa\\.conda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "# assert timm.__version__ == \"0.3.2\"  # version check\n",
    "import timm.optim.optim_factory as optim_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import util.misc as misc\n",
    "from util.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "from util.custom_dataset import CustomDataset\n",
    "import util.transform_npy as transform_npy\n",
    "\n",
    "\n",
    "import models_mae\n",
    "\n",
    "from engine_pretrain import train_one_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_parser():\n",
    "    # Create an argument parser named 'parser'\n",
    "    parser = argparse.ArgumentParser('MAE Pre-training', add_help=False)\n",
    "    # 'MAE Pre-training' is the description that will appear in the help message when the script is run\n",
    "    # 'add_help=False' disables the default help option (-h or --help) to allow hierarchical argument structure\n",
    "\n",
    "    # Define various arguments with their details using parser.add_argument()\n",
    "    parser.add_argument('--batch_size', default=64, type=int,\n",
    "                        help='Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus')\n",
    "    \n",
    "    parser.add_argument('--epochs', default=400, type=int)\n",
    "    \n",
    "    parser.add_argument('--accum_iter', default=1, type=int,\n",
    "                        help='Accumulate gradient iterations (for increasing effective batch size under memory constraints)')\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--model', default='mae_vit_large_patch16', type=str, metavar='MODEL',\n",
    "                        help='Name of model to train')\n",
    "\n",
    "    parser.add_argument('--input_size', default=224, type=int,\n",
    "                        help='Image input size')\n",
    "\n",
    "    parser.add_argument('--mask_ratio', default=0.75, type=float,\n",
    "                        help='Masking ratio (percentage of removed patches).')\n",
    "\n",
    "    parser.add_argument('--norm_pix_loss', action='store_true',\n",
    "                        help='Use normalized pixels (per patch) as targets for computing loss')\n",
    "    parser.set_defaults(norm_pix_loss=False)  # Set default value for 'norm_pix_loss' as False\n",
    "\n",
    "    # Optimizer parameters\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.05,\n",
    "                        help='Weight decay (default: 0.05)')\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=None, metavar='LR',\n",
    "                        help='Learning rate (absolute lr)')\n",
    "    \n",
    "    parser.add_argument('--blr', type=float, default=1e-3, metavar='LR',\n",
    "                        help='Base learning rate: absolute_lr = base_lr * total_batch_size / 256')\n",
    "    \n",
    "    parser.add_argument('--min_lr', type=float, default=0., metavar='LR',\n",
    "                        help='Lower LR bound for cyclic schedulers that hit 0')\n",
    "\n",
    "    parser.add_argument('--warmup_epochs', type=int, default=40, metavar='N',\n",
    "                        help='Epochs to warm up LR')\n",
    "\n",
    "    # Dataset parameters\n",
    "    parser.add_argument('--data_path', default='/datasets01/imagenet_full_size/061417/', type=str,\n",
    "                        help='Dataset path')\n",
    "\n",
    "    parser.add_argument('--output_dir', default='./output_dir',\n",
    "                        help='Path to save results, empty for no saving')\n",
    "    \n",
    "    parser.add_argument('--log_dir', default='./output_dir',\n",
    "                        help='Path to TensorBoard log')\n",
    "    \n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='Device to use for training / testing')\n",
    "    \n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "    \n",
    "    parser.add_argument('--resume', default='',\n",
    "                        help='Resume from checkpoint')\n",
    "\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='Start epoch')\n",
    "    \n",
    "    parser.add_argument('--num_workers', default=10, type=int)\n",
    "    \n",
    "    parser.add_argument('--pin_mem', action='store_true',\n",
    "                        help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "    parser.add_argument('--no_pin_mem', action='store_false', dest='pin_mem')\n",
    "    parser.set_defaults(pin_mem=True)  # Set default value for 'pin_mem' as True\n",
    "\n",
    "    # Distributed training parameters\n",
    "    parser.add_argument('--world_size', default=1, type=int,\n",
    "                        help='Number of distributed processes')\n",
    "    \n",
    "    parser.add_argument('--local_rank', default=-1, type=int)\n",
    "    \n",
    "    parser.add_argument('--dist_on_itp', action='store_true')\n",
    "    \n",
    "    parser.add_argument('--dist_url', default='env://',\n",
    "                        help='URL used to set up distributed training')\n",
    "\n",
    "    return parser  # Return the fully configured argument parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    # Initialize distributed mode if needed\n",
    "    misc.init_distributed_mode(args)  # Initialize distributed training if required\n",
    "\n",
    "    # Print job directory and arguments\n",
    "    print('job dir: {}'.format(os.path.dirname(os.path.realpath(__file__))))\n",
    "    print(\"{}\".format(args).replace(', ', ',\\n'))\n",
    "\n",
    "    # Set the device for training (e.g., 'cuda' or 'cpu')\n",
    "    device = torch.device(args.device)\n",
    "\n",
    "    # Fix the random seed for reproducibility\n",
    "    seed = args.seed + misc.get_rank()  # Combine provided seed and distributed rank\n",
    "    torch.manual_seed(seed)  # Set PyTorch random seed\n",
    "    np.random.seed(seed)  # Set NumPy random seed\n",
    "\n",
    "    cudnn.benchmark = True  # Enable CuDNN benchmark mode for optimized performance\n",
    "\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        transform_npy.ResizeNpyWithPadding((args.input_size, args.input_size)),\n",
    "        transform_npy.RandomHorizontalFlipNpy(),\n",
    "        transform_npy.RandomRotationNpy(degrees=(-15, 15)),\n",
    "        transforms.Lambda(lambda data: data.copy()),  # Copy the data to avoid grad error\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "])\n",
    "    \n",
    "    # Create a training dataset using the defined transformations\n",
    "    dataset_train = CustomDataset(data_path=os.path.join(args.data_path, 'train'), transform=transform_train)\n",
    "    print(dataset_train)\n",
    "\n",
    "    # Configure data sampler for distributed training (if applicable)\n",
    "    if True:  # args.distributed:\n",
    "        num_tasks = misc.get_world_size()\n",
    "        global_rank = misc.get_rank()\n",
    "        sampler_train = torch.utils.data.DistributedSampler(\n",
    "            dataset_train, num_replicas=num_tasks, rank=global_rank, shuffle=True\n",
    "        )\n",
    "        print(\"Sampler_train = %s\" % str(sampler_train))\n",
    "    else:\n",
    "        sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "\n",
    "    # Set up logging writer for TensorBoard\n",
    "    if global_rank == 0 and args.log_dir is not None:\n",
    "        os.makedirs(args.log_dir, exist_ok=True)\n",
    "        log_writer = SummaryWriter(log_dir=args.log_dir)\n",
    "    else:\n",
    "        log_writer = None\n",
    "\n",
    "    # Create data loader for training dataset\n",
    "    data_loader_train = torch.utils.data.DataLoader(\n",
    "        dataset_train, sampler=sampler_train,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=args.pin_mem,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    \n",
    "    # Define the neural model using the specified architecture\n",
    "    model = models_mae.__dict__[args.model](norm_pix_loss=args.norm_pix_loss)\n",
    "\n",
    "    # Move the model to the specified device\n",
    "    model.to(device)\n",
    "\n",
    "    # Calculate effective batch size for training\n",
    "    eff_batch_size = args.batch_size * args.accum_iter * misc.get_world_size()\n",
    "    \n",
    "    # Calculate learning rate based on batch size and base learning rate\n",
    "    if args.lr is None:  # If only base_lr is specified\n",
    "        args.lr = args.blr * eff_batch_size / 256\n",
    "\n",
    "    # Print learning rate and other training settings\n",
    "    print(\"base lr: %.2e\" % (args.lr * 256 / eff_batch_size))\n",
    "    print(\"actual lr: %.2e\" % args.lr)\n",
    "    print(\"accumulate grad iterations: %d\" % args.accum_iter)\n",
    "    print(\"effective batch size: %d\" % eff_batch_size)\n",
    "\n",
    "    # Set up DistributedDataParallel if using distributed training\n",
    "    if args.distributed:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu], find_unused_parameters=True)\n",
    "        model_without_ddp = model.module\n",
    "    \n",
    "    # Set weight decay for bias and norm layers following timm's recommendation\n",
    "    param_groups = optim_factory.add_weight_decay(model_without_ddp, args.weight_decay)\n",
    "    # Create an AdamW optimizer with specified parameters\n",
    "    optimizer = torch.optim.AdamW(param_groups, lr=args.lr, betas=(0.9, 0.95))\n",
    "    print(optimizer)\n",
    "    # Create a loss scaler for mixed-precision training\n",
    "    loss_scaler = NativeScaler()\n",
    "\n",
    "    # Load model checkpoint and optimizer state if available\n",
    "    misc.load_model(args=args, model_without_ddp=model_without_ddp, optimizer=optimizer, loss_scaler=loss_scaler)\n",
    "\n",
    "    # Start training loop for specified number of epochs\n",
    "    print(f\"Start training for {args.epochs} epochs\")\n",
    "    start_time = time.time()\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        # Set epoch for distributed training\n",
    "        if args.distributed:\n",
    "            data_loader_train.sampler.set_epoch(epoch)\n",
    "        \n",
    "        # Perform one epoch of training and get training statistics\n",
    "        train_stats = train_one_epoch(\n",
    "            model, data_loader_train,\n",
    "            optimizer, device, epoch, loss_scaler,\n",
    "            log_writer=log_writer,\n",
    "            args=args\n",
    "        )\n",
    "        \n",
    "        # Save model checkpoint and statistics periodically\n",
    "        if args.output_dir and (epoch % 20 == 0 or epoch + 1 == args.epochs):\n",
    "            misc.save_model(\n",
    "                args=args, model=model, model_without_ddp=model_without_ddp, optimizer=optimizer,\n",
    "                loss_scaler=loss_scaler, epoch=epoch)\n",
    "\n",
    "        # Prepare log statistics for logging\n",
    "        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                        'epoch': epoch,}\n",
    "\n",
    "        # Write log statistics to file if applicable\n",
    "        if args.output_dir and misc.is_main_process():\n",
    "            if log_writer is not None:\n",
    "                log_writer.flush()\n",
    "            with open(os.path.join(args.output_dir, \"log.txt\"), mode=\"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "    # Calculate total training time and print\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print('Training time {}'.format(total_time_str))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
